{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Theory/Computation Problems\n",
    "\n",
    "### Problem 1 (20 points) \n",
    "Show that the stationary point (zero gradient) of the function\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    f=2x_{1}^{2} - 4x_1 x_2+ 1.5x^{2}_{2}+ x_2\n",
    "\\end{aligned}\n",
    "$$\n",
    "is a saddle (with indefinite Hessian). Find the directions of downslopes away from the saddle. Hint: Use Taylor's expansion at the saddle point. Find directions that reduce $f$.\n",
    "\n",
    "### Problem 2 (50 points) \n",
    "\n",
    "* (10 points) Find the point in the plane $x_1+2x_2+3x_3=1$ in $\\mathbb{R}^3$ that is nearest to the point $(-1,0,1)^T$. Is this a convex problem? Hint: Convert the problem into an unconstrained problem using $x_1+2x_2+3x_3=1$.\n",
    "\n",
    "* (40 points) Implement the gradient descent and Newton's algorithm for solving the problem. Attach your codes along with a short summary including (1) the initial points tested, (2) corresponding solutions, (3) a log-linear convergence plot.\n",
    "\n",
    "### Problem 3 (10 points) \n",
    "Let $f(x)$ and $g(x)$ be two convex functions defined on the convex set $\\mathcal{X}$. \n",
    "* (5 points) Prove that $af(x)+bg(x)$ is convex for $a>0$ and $b>0$. \n",
    "* (5 points) In what conditions will $f(g(x))$ be convex?\n",
    "\n",
    "### Problem 4 (bonus 10 points)\n",
    "Show that $f({\\bf x}_1) \\geq f(\\textbf{x}_0) + \n",
    "    \\textbf{g}_{\\textbf{x}_0}^T(\\textbf{x}_1-\\textbf{x}_0)$ for a convex function $f(\\textbf{x}): \\mathcal{X} \\rightarrow \\mathbb{R}$ and for $\\textbf{x}_0$, $\\textbf{x}_1 \\in \\mathcal{X}$. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Design Problems\n",
    "\n",
    "### Problem 5 (20 points) \n",
    "Consider an illumination problem: There are $n$ lamps and $m$ mirrors fixed to the ground. The target reflection intensity level is $I_t$. The actual reflection intensity level on the $k$th mirror can be computed as $\\textbf{a}_k^T \\textbf{p}$, where $\\textbf{a}_k$ is given by the distances between all lamps to the mirror, and $\\textbf{p}:=[p_1,...,p_n]^T$ are the power output of the lamps. The objective is to keep the actual intensity levels as close to the target as possible by tuning the power output $\\textbf{p}$.\n",
    "\n",
    "* (5 points) Formulate this problem as an optimization problem. \n",
    "* (5 points) Is your problem convex?\n",
    "* (5 points) If we require the overall power output of any of the $n$ lamps to be less than $p^*$, will the problem have a unique solution?\n",
    "* (5 points) If we require no more than half of the lamps to be switched on, will the problem have a unique solution?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Problem 1.0.1 \n",
    "$$f = 2x_{1}^{2} - 4x_1x_2 + 1.5x_2^2$$\n",
    "The stationary point of the function is obtained by taking the gradient and setting it to zero: \\\n",
    "$ \\rho = \\begin{bmatrix}4x_1 - 4x_2 \\\\ -4x_1 + 3x_2 + 1 \\end{bmatrix}$ = 0\\\n",
    "Solving for x_1 and x_2 gives a stationary point (1,1). \\\n",
    "Indefiniteness of the Hessian can be shown by computing its eigenvalues: \\\n",
    "\n",
    "$ H = \\begin{bmatrix} 4 & -4 \\\\ -4 & -1 \\end{bmatrix} $ \\\n",
    "$ \\begin{vmatrix} 4 - \\lambda & -4 \\\\ -4 & -1 - \\lambda \\end{vmatrix} = 0 $ \\\n",
    "$ \\lambda_{1} = 10.54, \\lambda_{2} = -4.54 $ \\\n",
    "Since the eigenvalues are both positive and negative, the Hessian is indefinite. \\\n",
    "To find the direction of the slopes awy from the saddle point (1,1) we need to find the points such that the difference f - f(1,1) < 0: \\\n",
    "$ f - f(1,1) = \\frac{f_{x_1x_1}(1,1)(x_1 - 1)^2}{2} + \\frac{f_{x_1x_2}(1,1)(x_1 - 1)(x_2 - 1)}{2} + \\frac{f_{x_2x_2}(1,1)(x_2 - 1)^2}{2} $ \\\n",
    "$ f - 0.5 = 2(x_1 - 1)^2 - 2(x_1 - 1)(x_2 - 1) - 0.5(x_2 - 1)^2 < 0 $ \\ \n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Problem 2:\n",
    "Part a: (Analytical)\n",
    "The problem asks to find a point closest to (-1, 0, 1) such that it is constrained to lie on the plane $ x_1 + 2x_2 + 3x_3 = 1 $. \\\n",
    "The problem can be formulated as:\n",
    "$ \\underset{x}{\\operatorname{argmin}} L(x_0, x) \\\\\n",
    " \\text{st } x_1 + 2x_2 + 3x_3 = 1 $\n",
    "\\\n",
    "Using Lagrange multipliers on the equality constraint: \n",
    "\\\n",
    "$ \\underset{x}{\\operatorname{argmin}} L(x, x_0) = (x_{01} - x_1)^2 + (x_{02} - x_2)^2 + (x_{03} - x_3)^2 = \\lambda g(x) $ \\\n",
    "$ \\nabla L(x) = \\lambda \\nabla g(x) $ \\\n",
    "$ g(x) = c $ \\\n",
    "The constraint equation works out to be: \\\n",
    "$ \\nabla L(x) = \\begin{bmatrix}2x_1 + 2 \\\\ 2x_2 \\\\ 2x_3 - 2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ \\lambda \\\\ 2\\lambda \\end{bmatrix} $ \\\n",
    "Together with the equation $ x_1 + 2x_2 + 3x_3 - 1 = 0 $, solving for the x's we get:\\\n",
    "$ x = \\begin{bmatrix} -1 \\\\ \\frac{-1}{8} \\\\ \\frac{3}{4} \\end{bmatrix} $ \\\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Problem 3: \n",
    "(i) Let f(x) and g(x) be two convex functions defined on the convex set X. \n",
    "The linear combination h(x) is defined as: \\\n",
    "$ h(x) = af(x) + bg(x) $\n",
    "If f(x) and g(x) are convex then: \\\n",
    "$ f(\\lambda x_1 + (1- \\lambda)x_2) \\le \\lambda f(x_1) + (1 - \\lambda)f(x_2) $ \\\n",
    "$ g(\\lambda x_1 + (1- \\lambda)x_2) \\le \\lambda g(x_1) + (1 - \\lambda)g(x_2) $ \\\n",
    "$ af(\\lambda x_1 + (1- \\lambda)x_2) + bg(\\lambda x_1 + (1- \\lambda)x_2) \\le a(\\lambda f(x_1) + (1 - \\lambda)f(x_2)) + b(\\lambda g(x_1) + (1 - \\lambda)g(x_2)) $ \\\n",
    "The right side can be rewritten as: \\\n",
    "$ \\lambda(af(x_1) + bg(x_1)) + (1-\\lambda)(af(x_2) + bg(x_2)) $ \\\n",
    "From the definition of h(x), the right hand side can be transformed to: \\\n",
    "$ \\lambda h(x_1) + (1-\\lambda)h(x_2) $ \\\n",
    "$ \\square $ \\\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Problem 5 \n",
    "(i) Formulation of optimization problem:\n",
    "$ \\underset{p_j}{\\operatorname{min}} \\sum_{k = 1}^{n} (I_k - I_t)^2 $ \\\n",
    "st $ 0 \\le p_j \\le p_{max}$ \\\n",
    "(ii) A formulation is convex if the objective is a convex function and the constraints have a feasible set that is convex. In  this case, the objective is convex due to the fact that $ I_k - I_t = a_k^Tp - I_t $ which is a hyperplane, and squaring a hyperplane is a quadratic function that is also convex. Moreover, the summation of k convex functions is also convex. Therefore the objective is convex. The feasible set from the constraints is convex because they form a segmented space that is a hypercube. \\\n",
    "(iii) If we require the power output of any of the n lamps to be less than p*, the modified optimization problem is: \\\n",
    "$ \\underset{p_j}{\\operatorname{min}} \\sum_{k = 1}^{n} (I_k - I_t)^2 $ \\\n",
    "st $ 0 \\le p_j \\le p_{max}$ , \\\n",
    "$ \\sum_{j = 1}^{k} p_j < p^*,  k = {1,...,n}$ \\\n",
    "The second constraint can be further split into n separate constraints. Unique solutions are possible if all the constraints are convex sets. Consider the constraint $ \\sum_{j = 1}^{l} p_j \\lt p^* , l \\epsilon k $. The summation produces an l-dimensional half space of a hyperplane, which is a convex set. Therefore the problem has a unique solution.\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# PROBLEM 2 : CODE FOR GRADIENT DESCENT\n",
    "print(\"Gradient descent solution\")\n",
    "import numpy as np\n",
    "\n",
    "def objective(x):\n",
    "    return (2 - (2*x[0]) - (3*x[1]))**2 + (x[0])**2 + (x[1] - 1)**2\n",
    "\n",
    "def grad(x):\n",
    "    return np.asarray([10*x[0] + 12*x[1] - 8, 12*x[0] + 20*x[1] - 14])\n",
    "\n",
    "def phi(a,x):\n",
    "    obj = objective(x)\n",
    "    corr = a*np.asarray([g**2 for g in grad(x)])\n",
    "    return obj - corr\n",
    "\n",
    "def line_search(x):\n",
    "    a = 1  # initialize step size\n",
    "    while np.linalg.norm(phi(a,x))<objective(x-a*grad(x)):  # if f(x+a*d)>phi(a) then backtrack. d is the search direction\n",
    "        a = 0.5*a\n",
    "    return a\n",
    "\n",
    "eps = 1e-3\n",
    "x0 = [0,0]\n",
    "k = 0\n",
    "x = x0\n",
    "\n",
    "error = np.linalg.norm(grad(x))\n",
    "\n",
    "while error >= eps and k < 10000:  # keep searching while gradient norm is larger than eps\n",
    "    a = line_search(x)\n",
    "    x = x - a*grad(x)\n",
    "    error = np.linalg.norm(grad(x))\n",
    "    k+=1\n",
    "    if k % 1000 == 0: print(\"Iteration:\", k, \"alpha:\", a, \"Error:\", error, \"x:\", x)\n",
    "\n",
    "x1 = 1 - 2*x[0] - 3*x[1]\n",
    "x = [x1, x[0], x[1]]\n",
    "print(\"Final solution:\", x)\n",
    "\n",
    "###########################################\n",
    "# Problem 2: Newton's method\n",
    "print(\"Newton's method solution: \")\n",
    "import numpy as np\n",
    "\n",
    "def objective(x):\n",
    "    return (2 - (2*x[0]) - (3*x[1]))**2 + (x[0])**2 + (x[1] - 1)**2\n",
    "\n",
    "def grad(x):\n",
    "    return np.asarray([10*x[0] + 12*x[1] - 8, 12*x[0] + 20*x[1] - 14])\n",
    "\n",
    "def hessian():\n",
    "    return np.asarray([[10,12],[12,20]])\n",
    "\n",
    "def phi(a,x):\n",
    "    obj = objective(x)\n",
    "    corr = a*np.asarray([g**2 for g in grad(x)])\n",
    "    return obj - corr\n",
    "\n",
    "def line_search(x):\n",
    "    a = 1  # initialize step size\n",
    "    while np.linalg.norm(phi(a,x))<objective(x-a*grad(x)):  # if f(x+a*d)>phi(a) then backtrack. d is the search direction\n",
    "        a = 0.5*a\n",
    "    return a\n",
    "\n",
    "eps = 1e-3\n",
    "x0 = [0,0]\n",
    "k = 0\n",
    "x = np.asarray(x0).reshape(2,1)\n",
    "\n",
    "lam = np.matmul(np.matmul(grad(x).reshape(1,2),np.linalg.inv(hessian())), grad(x).reshape(2,1))\n",
    "while lam**2/2 >= eps and k < 1000:  # keep searching while gradient norm is larger than eps\n",
    "    a = line_search(x)\n",
    "    x = x - a*np.matmul(np.linalg.inv(hessian()), grad(x).reshape(2,1))\n",
    "    lam = np.matmul(np.matmul(grad(x).reshape(1,2),np.linalg.inv(hessian())), grad(x).reshape(2,1))\n",
    "    k+=1\n",
    "    if k % 100 == 0: print(\"Iteration:\", k, \"alpha:\", a, \"Error:\", error, \"x:\", x)\n",
    "\n",
    "x1 = 1 - 2*x[0] - 3*x[1]\n",
    "x = [x1, x[0], x[1]]\n",
    "print(\"Final solution:\", x)\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Gradient descent solution\n",
      "Iteration: 1000 alpha: 0.125 Error: 0.8604186897840943 x: [-0.12581163  0.81128256]\n",
      "Iteration: 2000 alpha: 0.125 Error: 0.9204354759534935 x: [-0.12462265  0.81306602]\n",
      "Iteration: 3000 alpha: 0.125 Error: 0.9846386130992997 x: [-0.12335074  0.81497389]\n",
      "Iteration: 4000 alpha: 0.0625 Error: 0.3159960335302552 x: [-0.13659703  0.79510445]\n",
      "Iteration: 5000 alpha: 0.0625 Error: 0.3380377053348803 x: [-0.13616037  0.79575944]\n",
      "Iteration: 6000 alpha: 0.0625 Error: 0.36161684990617876 x: [-0.13569325  0.79646012]\n",
      "Iteration: 7000 alpha: 0.0625 Error: 0.38684071058438907 x: [-0.13519355  0.79720968]\n",
      "Iteration: 8000 alpha: 0.0625 Error: 0.4138240112546796 x: [-0.13465899  0.79801151]\n",
      "Iteration: 9000 alpha: 0.0625 Error: 0.442689478137391 x: [-0.13408714  0.79886928]\n",
      "Iteration: 10000 alpha: 0.0625 Error: 0.47356839797524564 x: [-0.13347541  0.79978688]\n",
      "Final solution: [-1.1324098264763962, -0.13347541131132373, 0.7997868830330145]\n",
      "Newton's method solution: \n",
      "Final solution: [array([-0.94053665]), array([-0.13383011]), array([0.73606562])]\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "interpreter": {
   "hash": "283a19fe1c80cc8d674e9be3676f3725f85ae6255e6112c9eba9160b54893d27"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}