{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "touched-logic",
   "metadata": {},
   "source": [
    "# Theory/Computation Problems\n",
    "\n",
    "### Problem 1 (20 points) \n",
    "Show that the stationary point (zero gradient) of the function\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    f=2x_{1}^{2} - 4x_1 x_2+ 1.5x^{2}_{2}+ x_2\n",
    "\\end{aligned}\n",
    "$$\n",
    "is a saddle (with indefinite Hessian). Find the directions of downslopes away from the saddle. Hint: Use Taylor's expansion at the saddle point. Find directions that reduce $f$.\n",
    "\n",
    "### Problem 2 (50 points) \n",
    "\n",
    "* (10 points) Find the point in the plane $x_1+2x_2+3x_3=1$ in $\\mathbb{R}^3$ that is nearest to the point $(-1,0,1)^T$. Is this a convex problem? Hint: Convert the problem into an unconstrained problem using $x_1+2x_2+3x_3=1$.\n",
    "\n",
    "* (40 points) Implement the gradient descent and Newton's algorithm for solving the problem. Attach your codes along with a short summary including (1) the initial points tested, (2) corresponding solutions, (3) a log-linear convergence plot.\n",
    "\n",
    "### Problem 3 (10 points) \n",
    "Let $f(x)$ and $g(x)$ be two convex functions defined on the convex set $\\mathcal{X}$. \n",
    "* (5 points) Prove that $af(x)+bg(x)$ is convex for $a>0$ and $b>0$. \n",
    "* (5 points) In what conditions will $f(g(x))$ be convex?\n",
    "\n",
    "### Problem 4 (bonus 10 points)\n",
    "Show that $f({\\bf x}_1) \\geq f(\\textbf{x}_0) + \n",
    "    \\textbf{g}_{\\textbf{x}_0}^T(\\textbf{x}_1-\\textbf{x}_0)$ for a convex function $f(\\textbf{x}): \\mathcal{X} \\rightarrow \\mathbb{R}$ and for $\\textbf{x}_0$, $\\textbf{x}_1 \\in \\mathcal{X}$. \n",
    "\n",
    "    \n",
    "# Design Problems\n",
    "\n",
    "### Problem 5 (20 points) \n",
    "Consider an illumination problem: There are $n$ lamps and $m$ mirrors fixed to the ground. The target reflection intensity level is $I_t$. The actual reflection intensity level on the $k$th mirror can be computed as $\\textbf{a}_k^T \\textbf{p}$, where $\\textbf{a}_k$ is given by the distances between all lamps to the mirror, and $\\textbf{p}:=[p_1,...,p_n]^T$ are the power output of the lamps. The objective is to keep the actual intensity levels as close to the target as possible by tuning the power output $\\textbf{p}$.\n",
    "\n",
    "* (5 points) Formulate this problem as an optimization problem. \n",
    "* (5 points) Is your problem convex?\n",
    "* (5 points) If we require the overall power output of any of the $n$ lamps to be less than $p^*$, will the problem have a unique solution?\n",
    "* (5 points) If we require no more than half of the lamps to be switched on, will the problem have a unique solution?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joint-machinery",
   "metadata": {},
   "source": [
    "# Solutions\n",
    "\n",
    "### Problem 1 solution\n",
    "\n",
    "Find a stationary point\n",
    "\n",
    "$$\n",
    "                \\begin{aligned}\n",
    "                    &\\frac{\\partial f}{\\partial x_1} = 4x_1-4x_2=0\\\\\n",
    "                    &\\frac{\\partial f}{\\partial x_2} = -4x_1+3x_2+1=0\n",
    "                \\end{aligned}\n",
    "$$\n",
    "\n",
    "Solve these to get the stationary point $x_* = [1,1]^T$.\n",
    "\n",
    "Calculate the Hessian to get $H=[4, -4; -4, 3]$. It is indefinite since one eigenvalue is positive and the other is negative. So the stationary point is a saddle point.\n",
    "        \n",
    "To find the direction of downslope, denote $\\partial \\textbf{x}_*=\\textbf{x}-\\textbf{x}_*$ and $\\partial f_* = f(\\textbf{x})-f(\\textbf{x}_*)$: \n",
    "\n",
    "$$\n",
    "            \\begin{aligned}\n",
    "                \\partial f_* & = \\nabla f_* \\partial \\textbf{x}_* + \\frac{1}{2} \\partial \\textbf{x}_*^T \\textbf{H} \\partial\\textbf{x}_* \\\\\n",
    "                & = \\frac{1}{2}(2\\partial x_1 -\\partial x_2)(2\\partial x_1 -3\\partial x_2)\n",
    "            \\end{aligned}\n",
    "$$\n",
    "\n",
    "Set $\\partial f_*<0$ to get the downslopes $2\\partial x_1 -\\partial x_2<0$ and $2\\partial x_1 -3\\partial x_2>0$ or $2\\partial x_1 -\\partial x_2>0$ and $2\\partial x_1 -3\\partial x_2<0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-india",
   "metadata": {},
   "source": [
    "### Problem 2 solution\n",
    "\n",
    "Solve the following problem\n",
    "$$\n",
    "            \\begin{aligned}\n",
    "                \\min_{x_1,x_2,x_3} & \\quad (x_1+1)^2+x_2^2 + (x_3-1)^2 \\\\\n",
    "                \\text{subject to:} & \\quad x_1+2x_2+3x_3=1 \n",
    "            \\end{aligned}\n",
    "$$\n",
    "Substituting $x_1=1-2x_2-3x_3$, the problem reduces to an unconstrained optimization problem. The solution is $x_1=-15/14,x_2=-1/7,x_3=11/14$.\n",
    "\n",
    "The unconstrained problem has positive definite Hessian everywhere. The problem is thus convex. You can also show that the original problem has a convex objective function and a convex feasible solution set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "dangerous-clinic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 0]),\n",
       " array([0.0625  , 0.109375]),\n",
       " array([0.10986328, 0.19580078]),\n",
       " array([0.14542389, 0.26428223]),\n",
       " array([0.17178619, 0.31872964]),\n",
       " array([0.19098449, 0.36219818]),\n",
       " array([0.20460775, 0.39707492]),\n",
       " array([0.21389699, 0.42522498]),\n",
       " array([0.2257459 , 0.47098649]),\n",
       " array([0.22716314, 0.50022586]),\n",
       " array([0.22287655, 0.52006219]),\n",
       " array([0.20820431, 0.54894461]),\n",
       " array([0.124532  , 0.61427662]),\n",
       " array([0.08599204, 0.62803184]),\n",
       " array([0.03645422, 0.67896418]),\n",
       " array([0.02045071, 0.67844123]),\n",
       " array([-0.02277453,  0.70166208]),\n",
       " array([-0.03478701,  0.71666537]),\n",
       " array([-0.05054416,  0.72192391]),\n",
       " array([-0.06039699,  0.73242714]),\n",
       " array([-0.08354146,  0.74195478]),\n",
       " array([-0.0856678 ,  0.74706109]),\n",
       " array([-0.09917469,  0.75791006]),\n",
       " array([-0.10562305,  0.7599035 ]),\n",
       " array([-0.10953627,  0.76424141]),\n",
       " array([-0.11897805,  0.76794228]),\n",
       " array([-0.11977577,  0.77009513]),\n",
       " array([-0.12519875,  0.77452096]),\n",
       " array([-0.12784025,  0.77526882]),\n",
       " array([-0.12939171,  0.77706298]),\n",
       " array([-0.13131913,  0.77777804]),\n",
       " array([-0.13383727,  0.78031164]),\n",
       " array([-0.13462999,  0.78030584]),\n",
       " array([-0.13680127,  0.78148622]),\n",
       " array([-0.13741514,  0.78222939]),\n",
       " array([-0.13820272,  0.78250401]),\n",
       " array([-0.13870403,  0.78302604]),\n",
       " array([-0.13986305,  0.78351698]),\n",
       " array([-0.13997472,  0.78376751]),\n",
       " array([-0.14065759,  0.78431081]),\n",
       " array([-0.1409797 ,  0.78441549]),\n",
       " array([-0.14117901,  0.7846309 ]),\n",
       " array([-0.1416516 ,  0.78482215]),\n",
       " array([-0.14169378,  0.78492766]),\n",
       " array([-0.14196804,  0.78514919]),\n",
       " array([-0.14209991,  0.78518873]),\n",
       " array([-0.14217902,  0.78527775]),\n",
       " array([-0.14227544,  0.78531483]),\n",
       " array([-0.14240338,  0.78544092]),\n",
       " array([-0.14244267,  0.78544161])]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample code for gradient descent\n",
    "\n",
    "import numpy as np\n",
    "obj = lambda x: (2 - 2*x[0] - 3*x[1])**2 + x[0]**2 + (x[1]-1)**2# note that this is 1D. In Prob 2 it should be 2D.\n",
    "def grad(x):\n",
    "     return np.array([10*x[0] + 12*x[1] - 8, 20*x[1] + 12*x[0] -14])\n",
    "eps = 1e-3  # termination criterion\n",
    "x0= np.array([0,0])  # initial guess\n",
    "k = 0  # counter\n",
    "soln = [x0]  # use an array to store the search steps\n",
    "x = soln[k]  # start with the initial guess\n",
    "error = np.linalg.norm(grad(x))  # compute the error. Note you will need to compute the norm for 2D grads, rather than the absolute value\n",
    "a = 0.01  # set a fixed step size to start with\n",
    "\n",
    "# Armijo line search\n",
    "def line_search(x, d):\n",
    "    a = 1.  # initialize step size\n",
    "    \n",
    "    def phi(a,x,d):\n",
    "        return obj(x)+a*0.8*np.dot(grad(x),d)\n",
    "\n",
    "    while phi(a,x,d)<obj(x+a*d):  # while phi(a,x)<obj(x-a*grad(x)): # if f(x+a*d)>phi(a) then backtrack. d is the search direction\n",
    "        a = 0.5*a\n",
    "        \n",
    "    return a\n",
    "\n",
    "while error >= eps:  # keep searching while gradient norm is larger than eps\n",
    "    d = -grad(x)\n",
    "    \n",
    "    a = line_search(x, d)\n",
    "    \n",
    "    x = x+a*d\n",
    "    soln.append(x)\n",
    "    error = np.linalg.norm(grad(x))\n",
    "#     error\n",
    "soln  # print the search trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-rough",
   "metadata": {},
   "source": [
    "### Problem 3 solution\n",
    "\n",
    "For any two points $x_1$ and $x_2$, we have \n",
    "\n",
    "$$\n",
    "            f(\\lambda x_1 + (1-\\lambda)x_2) \\leq \\lambda f(x_1) + (1-\\lambda) f(x_2)\n",
    "$$\n",
    "and \n",
    "$$\n",
    "            g(\\lambda x_1 + (1-\\lambda)x_2) \\leq \\lambda g(x_1) + (1-\\lambda) g(x_2).\n",
    "$$\n",
    "Then \n",
    "$$\n",
    "            \\begin{aligned}\n",
    "            & af(\\lambda x_1 + (1-\\lambda)x_2)+bg(\\lambda x_1 + (1-\\lambda)x_2) \\\\  \n",
    "            & \\leq a(\\lambda f(x_1) + (1-\\lambda) f(x_2)) + b(\\lambda g(x_1) + (1-\\lambda) g(x_2))\\\\\n",
    "            & = \\lambda (af(x_1)+bg(x_1)) + (1-\\lambda)(af(x_2)+bg(x_2)).\n",
    "            \\end{aligned}\n",
    "$$\n",
    "Therefore by definition $af(x)+bg(x)$ is convex. \n",
    "\n",
    "The second-order derivative is $f''(g')^2+g''f'$. If $f''(g')^2+g''f'>0$ then $f(g)$ is convex. One special case will be when $f$ is monotonically increasing, i.e., $f'>0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collected-carbon",
   "metadata": {},
   "source": [
    "### Problem 4 solution\n",
    "\n",
    "The following is a proof for a 1D case. \n",
    "\n",
    "#### Necessity: \n",
    "\n",
    "If $f$ is convex, we have\n",
    "$$\n",
    "        f(x + \\lambda(y-x)) \\leq (1-\\lambda)f(x)+\\lambda f(y),\n",
    "$$\n",
    "    for $\\lambda \\in [0,1]$. Divide both sides by $\\lambda$ to have:\n",
    "$$\n",
    "        f(y) \\geq f(x) + \\frac{f(x + \\lambda(y-x))-f(x)}{\\lambda}.\n",
    "$$\n",
    "\tTake $t \\rightarrow 0$ to get $f(y) \\geq f(x) + \n",
    "    f'(x)(y-x)$.\n",
    "    \n",
    "#### Sufficiency: \n",
    "\n",
    "Let $z = \\lambda x + (1-\\lambda)y$ for $\\lambda \\in [0,1]$. We have\n",
    "    $f(x) \\geq f(z) + f'(z)(x-z)$, $f(y) \\geq f(z) + f'(z)(y-z)$. Multiplying the first inequality by $\\lambda$, the second by $1-\\lambda$, add the two together to have\n",
    "    $\\lambda f(x) + (1-\\lambda) f(y) \\geq f(z)$. Thus $f$ is convex.\n",
    "    \n",
    "For a general case in $\\mathbb{R}^n$, please see Page 70 ``Proof of first-order convexity condition'' in *Convex Optimization* by Stephen Boyd."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-satin",
   "metadata": {},
   "source": [
    "### Problem 5 solution\n",
    "\n",
    "\n",
    "The problem can be formulated as\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{\\textbf{p}} \\quad & \\sum_{k=1}^m \\left(\\textbf{a}_k^T \\textbf{p} - I_t\\right)^2\\\\\n",
    "\\text{s.t.} \\quad & 0 \\leq p_i \\leq p_{max} ~\\forall i=1,...,n\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The Hessian of the objective function is\n",
    "\n",
    "$$\n",
    "H = 2\\sum_{k=1}^m \\textbf{a}_k \\textbf{a}_k^T\n",
    "$$\n",
    "\n",
    "The problem has a convex feasible domain since constraints are simple bounds.\n",
    "\n",
    "$H$ is positive semi-definite when $\\{\\textbf{a}_k\\}_{k=1}^m$ does not span $\\mathbb{R}^n$, in which case the problem is convex but not strictly convex, or otherwise $H$ is positive defininte, in which case the problem is strictly convex.\n",
    "\n",
    "Adding the constraint $\\sum_{k=1}^n p_i \\leq p^*$ does not change the convexity of the feasible domain, since this constraint is linear in $\\textbf{p}$.\n",
    "\n",
    "Adding the constraint $\\sum_{k=1}^n \\mathbb{1}(p_i > 0) \\leq n/2$, where $\\mathbb{1}(condition)$ returns 1 if condition is true or otherwise 0, makes the feasible domain non-convex. To see this, consider the case with two lamps ($n=2$). $(p_1, p_2)=(1,0)$ and $(p_1,p_2)=(0,1)$ are feasible (one lamp on and the other off). But $(p_1,p_2)=(0.5, 0.5)$, which is along the line segment between $(1,0)$ and $(0,1)$, is not since both lamps are on in this case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
