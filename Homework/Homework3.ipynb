{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Problem 1 (50 points) \n",
    "\n",
    "Vapor-liquid equilibria data are correlated using two adjustable parameters $A_{12}$ and $A_{21}$ per binary\n",
    "mixture. For low pressures, the equilibrium relation can be formulated as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p = & x_1\\exp\\left(A_{12}\\left(\\frac{A_{21}x_2}{A_{12}x_1+A_{21}x_2}\\right)^2\\right)p_{water}^{sat}\\\\\n",
    "& + x_2\\exp\\left(A_{21}\\left(\\frac{A_{12}x_1}{A_{12}x_1+A_{21}x_2}\\right)^2\\right)p_{1,4 dioxane}^{sat}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Here the saturation pressures are given by the Antoine equation\n",
    "\n",
    "$$\n",
    "\\log_{10}(p^{sat}) = a_1 - \\frac{a_2}{T + a_3},\n",
    "$$\n",
    "\n",
    "where $T = 20$($^{\\circ}{\\rm C}$) and $a_{1,2,3}$ for a water - 1,4 dioxane\n",
    "system is given below.\n",
    "\n",
    "|             | $a_1$     | $a_2$      | $a_3$     |\n",
    "|:------------|:--------|:---------|:--------|\n",
    "| Water       | 8.07131 | 1730.63  | 233.426 |\n",
    "| 1,4 dioxane | 7.43155 | 1554.679 | 240.337 |\n",
    "\n",
    "\n",
    "The following table lists the measured data. Recall that in a binary system $x_1 + x_2 = 1$.\n",
    "\n",
    "|$x_1$ | 0.0 | 0.1 | 0.2 | 0.3 | 0.4 | 0.5 | 0.6 | 0.7 | 0.8 | 0.9 | 1.0 |\n",
    "|:-----|:--------|:---------|:--------|:-----|:-----|:-----|:-----|:-----|:-----|:-----|:-----|\n",
    "|$p$| 28.1 | 34.4 | 36.7 | 36.9 | 36.8 | 36.7 | 36.5 | 35.4 | 32.9 | 27.7 | 17.5 |\n",
    "\n",
    "Estimate $A_{12}$ and $A_{21}$ using data from the above table: \n",
    "\n",
    "1. Formulate the least square problem; \n",
    "2. Since the model is nonlinear, the problem does not have an analytical solution. Therefore, solve it using the gradient descent or Newton's method implemented in HW1; \n",
    "3. Compare your optimized model with the data. Does your model fit well with the data?\n",
    "\n",
    "---\n",
    "\n",
    "### Problem 2 (50 points) \n",
    "\n",
    "Solve the following problem using Bayesian Optimization:\n",
    "$$\n",
    "    \\min_{x_1, x_2} \\quad \\left(4-2.1x_1^2 + \\frac{x_1^4}{3}\\right)x_1^2 + x_1x_2 + \\left(-4 + 4x_2^2\\right)x_2^2,\n",
    "$$\n",
    "for $x_1 \\in [-3,3]$ and $x_2 \\in [-2,2]$. A tutorial on Bayesian Optimization can be found [here](https://thuijskens.github.io/2016/12/29/bayesian-optimisation/).\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Problem 1 : Least squares fitting for vapor-liquid equilibrium\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SMOOTH = 1e-5\n",
    "\n",
    "\n",
    "def model(X, A, p_sat):\n",
    "    # Question : Temporary variables and the computational graph ...\n",
    "    k1 = A[0]*(A[1]*X[1]/(A[0]*X[0] + A[1]*X[1]))\n",
    "    k2 = A[1]*(A[0]*X[0]/(A[0]*X[0] + A[1]*X[1]))\n",
    "    t1 = X[0]*torch.exp((k1**2))*p_sat[0]\n",
    "    t2 = X[1]*torch.exp((k2**2))*p_sat[1]\n",
    "    return t1 + t2\n",
    "\n",
    "\n",
    "# Define the variables to be optimized over\n",
    "A = torch.tensor([2.5,2.8], requires_grad = True, dtype = torch.float64)\n",
    "# Data\n",
    "X = np.asarray([np.arange(0,1.1,0.1), 1-np.arange(0,1.1,0.1)])\n",
    "X = torch.from_numpy(X)\n",
    "p = torch.tensor([28.1, 34.4, 36.7, 36.9, 36.8, 36.7, 36.5, 35.4, 32.9, 27.8, 17.5], requires_grad =  False, dtype = torch.float64)\n",
    "\n",
    "# Constants\n",
    "# a = torch.tensor([[8.07131, 1730.63, 233.426],[7.43155, 1554.679, 240.337]], requires_grad= False)\n",
    "# T = 20\n",
    "# p_sat_water = torch.tensor(a[0][0] - (a[0][1]/(T + a[0][2])))\n",
    "# p_sat_dioxane = torch.tensor(a[1][0] - (a[1][1]/(T + a[1][2])))\n",
    "p_sat = torch.tensor([1.2424, 1.4598], requires_grad = False, dtype = torch.float64)\n",
    "\n",
    "alpha = 0.0001\n",
    "for i in range(500):\n",
    "    # Define the loss function\n",
    "    loss = torch.sum((p - model(X, A, p_sat))**2)\n",
    "    loss.backward()\n",
    "    print(\"Alpha:\", alpha, \"Loss:\", loss.data.numpy(), \"Gradient:\", A.grad.numpy(), \"Parameter:\", A.data.numpy())\n",
    "    with torch.no_grad():\n",
    "        A -= alpha * A.grad\n",
    "        A.grad.zero_()\n",
    "    alpha /= 1.1\n",
    "    \n",
    "p_final = model(X, A, p_sat)\n",
    "print(\"Model: \", p_final)\n",
    "print(\"Truth: \", p)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Problem 2: Bayesian Optimization\n",
    "import sklearn.gaussian_process as gp\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def objective(x, mean = 0, var = 0.1):\n",
    "    '''\n",
    "    x : 2x1 vector of a sample in the domain of interest\n",
    "    mean : scalar mean for iid noise term\n",
    "    var : scalar variance for iid noise term\n",
    "    '''\n",
    "    noise = np.random.normal(mean, var)    \n",
    "    return ((4 - (2.1*(x[0]**2)) + ((x[0]**4)/3))*(x[0]**2)) + (x[0]*x[1]) + ((-4 + 4*x[1]**2)*x[1]**2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def expected_improvement(X_, X, model, k = 0.01):\n",
    "    Y_mu, Y_sigma = model.predict(X, return_std = True)\n",
    "    Y__= model.predict(X_)\n",
    "    \n",
    "    # Find the best value of predicted f(x) among the old samples\n",
    "    with np.errstate(divide='ignore'):\n",
    "        y_best = np.max(Y_sigma)\n",
    "        Z = (Y_mu - y_best - k)/Y_sigma\n",
    "        ei = ((Y_mu - y_best - k)*norm.cdf(Z)) + (Y_sigma*norm.pdf(Z))\n",
    "        ei[Y_sigma == 0.0] = 0.0\n",
    "    return X_[np.argmax(ei)]\n",
    "\n",
    "\n",
    "# optimize the expected improvement\n",
    "# EI function is highly non linear\n",
    "\n",
    "x1 = np.random.uniform(-3,3,5)\n",
    "x2 = np.random.uniform(-2,2,5)\n",
    "X = np.asarray([[x1],[x2]]).transpose().squeeze(1)\n",
    "Y = np.asarray([objective(x) for x in X])\n",
    "\n",
    "kernel = gp.kernels.RBF()\n",
    "model = gp.GaussianProcessRegressor(kernel=kernel,\n",
    "                                    alpha=1e-4,\n",
    "                                    n_restarts_optimizer=0,\n",
    "                                    normalize_y=True)\n",
    "\n",
    "n_iters = 10\n",
    "k = 0.01\n",
    "for i in range(n_iters):\n",
    "    model.fit(X,Y)\n",
    "    # Select next point using expected improvement\n",
    "    x1 = np.random.uniform(-3,3,5)\n",
    "    x2 = np.random.uniform(-2,2,5)\n",
    "    X_ = np.asarray([[x1],[x2]]).transpose().squeeze(1)\n",
    "    x_next = expected_improvement(X_ = X_, X = X, model = model)\n",
    "    y_next = objective(x_next)\n",
    "    X.stack(x_next)\n",
    "    Y.append(y_next)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "interpreter": {
   "hash": "283a19fe1c80cc8d674e9be3676f3725f85ae6255e6112c9eba9160b54893d27"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}