{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework_III.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HP_hgIKVXY1e"
      },
      "source": [
        "from math import exp\n",
        "import matplotlib.pyplot as plt\n",
        "import torch as t\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "from bayes_opt import BayesianOptimization\n",
        "# The BayesianOptimization library can be found here https://github.com/fmfn/BayesianOptimization\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzlotNQ7XrLZ"
      },
      "source": [
        "def problem_I():\n",
        "    # Equilibrium Relation (A12 and A21 are the unknowns)\n",
        "    # p = x1*exp(A12*(A21*x2/(A12*x1+A21*x2))**2)*pw + x2*exp(A21*(A12*x1/(A12*x1 + A21*x2))**2)*pd\n",
        "    # p = x1*t.exp(g[0]*(g[1]*x2/(g[0]*x1+g[1]*x2))**2)*pw + x2*t.exp(g[1]*(g[0]*x1/(g[0]*x1 + g[1]*x2))**2)*pd\n",
        "    # Use gradient descent to calculate A12 and A21 given the measured data\n",
        "\n",
        "    # Antoine Equation\n",
        "    T = 20\n",
        "    a1_w = 8.07131\n",
        "    a2_w = 1730.63\n",
        "    a3_w = 233.426\n",
        "    a1_d = 7.43155\n",
        "    a2_d = 1554.679\n",
        "    a3_d = 240.337\n",
        "    pw = 10**(a1_w - (a2_w/(T+a3_w)))\n",
        "    pd = 10**(a1_d - (a2_d/(T+a3_d)))\n",
        "\n",
        "    # Measured Data\n",
        "    x1 = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
        "    p_measured = [28.1, 34.4, 36.7, 36.9, 36.8, 36.7, 36.5, 35.4, 32.9, 27.7, 17.5]\n",
        "    x2 = []\n",
        "    for i in range(len(x1)):\n",
        "        x2.append(round(1 - x1[i], 2))\n",
        "\n",
        "\n",
        "    # Here is a code for gradient descent without line search\n",
        "    # Initial guesses for A12 and A21, respectively\n",
        "    g = Variable(t.tensor([1.0, 0.0]), requires_grad=True)\n",
        "\n",
        "    # Fix the step size\n",
        "    a = 0.001\n",
        "\n",
        "    # Acceptable error\n",
        "    error = 10**-10\n",
        "    loss = 1\n",
        "\n",
        "    # Start gradient descent\n",
        "    # Termination Criterion: If norm of gradient is larger than a certain error, then continue...\n",
        "    while loss > error:  # TODO: change the termination criterion\n",
        "        p = x1[1]*t.exp(g[0]*(g[1]*x2[1]/(g[0]*x1[1]+g[1]*x2[1]))**2)*pw + x2[1]*t.exp(g[1]*(g[0]*x1[1]/(g[0]*x1[1] + g[1]*x2[1]))**2)*pd\n",
        "        loss = (p_measured[1] - p)**2\n",
        "        loss.backward()\n",
        "\n",
        "        # no_grad() specifies that the operations within this context are not part of the computational graph, i.e., we don't need the gradient descent algorithm itself to be differentiable with respect to x\n",
        "        with t.no_grad():\n",
        "            g -= a * g.grad\n",
        "\n",
        "            # need to clear the gradient at every step, or otherwise it will accumulate...\n",
        "            g.grad.zero_()\n",
        "\n",
        "    print(g.data.numpy())\n",
        "    print(loss.data.numpy())\n",
        "\n",
        "    # Obtained optimized values fro A_12 and A_21\n",
        "    A_12 = g.data.numpy()[0]\n",
        "    A_21 = g.data.numpy()[1]\n",
        "\n",
        "\n",
        "    p_opt = []\n",
        "    # Calculate p using optimized A12 and A21 values\n",
        "    for i in range(len(x1)):\n",
        "        p_opt.append(x1[i]*exp(A_12*(A_21*x2[i]/(A_12*x1[i]+A_21*x2[i]))**2)*pw + x2[i]*exp(A_21*(A_12*x1[i]/(A_12*x1[i] + A_21*x2[i]))**2)*pd)\n",
        "\n",
        "    print(p_measured)\n",
        "    print(p_opt)\n",
        "    plt.scatter(x1, p_measured)\n",
        "    plt.scatter(x1, p_opt)\n",
        "    plt.ylabel('pressure')\n",
        "    plt.xlabel('x1')\n",
        "    plt.legend(['p_measured', 'p_optimized'])\n",
        "    plt.show()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Rf_jyfyXszg"
      },
      "source": [
        "def problem_II():\n",
        "    def function(x, y):\n",
        "        return -((4 - 2.1*x**2 + x**4/3)*x**2 + x*y + (-4 + 4*y**2)*y**2)\n",
        "\n",
        "    # Bounded region of parameter space\n",
        "    pbounds = {'x': (-3, 3), 'y': (-2, 2)}\n",
        "\n",
        "    optimizer = BayesianOptimization(\n",
        "        f=function,\n",
        "        pbounds=pbounds,\n",
        "        random_state=1,\n",
        "    )\n",
        "\n",
        "\n",
        "    optimizer.maximize(\n",
        "        init_points=5,\n",
        "        n_iter=100,\n",
        "    )\n",
        "\n",
        "    print(optimizer.max)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oZ5BMOiXwQx"
      },
      "source": [
        "def main():\n",
        "    problem_I()\n",
        "    problem_II()\n",
        "\n",
        "\n",
        "main()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}